{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Setup\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from trainer import fit\n",
    "import numpy as np\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "def plot_embeddings(embeddings, targets, class_labels, title=None, xlim=None, ylim=None):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    for i in range(10):\n",
    "        inds = np.where(targets==i)[0]\n",
    "        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])\n",
    "    if xlim:\n",
    "        plt.xlim(xlim[0], xlim[1])\n",
    "    if ylim:\n",
    "        plt.ylim(ylim[0], ylim[1])\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.legend(class_labels, loc='upper right')\n",
    "\n",
    "def extract_embeddings(dataloader, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        embeddings = np.zeros((len(dataloader.dataset), 2))\n",
    "        labels = np.zeros(len(dataloader.dataset))\n",
    "        k = 0\n",
    "        for images, target in dataloader:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n",
    "            labels[k:k+len(images)] = target.numpy()\n",
    "            k += len(images)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNSIST dataset setup\n",
    "\n",
    "mean, std = 0.1307, 0.3081\n",
    "n_classes = 10\n",
    "class_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "train_dataset = MNIST('MNIST', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((mean,), (std,))\n",
    "                             ]))\n",
    "test_dataset = MNIST('MNIST', train=False, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((mean,), (std,))\n",
    "                            ]))\n",
    "\n",
    "n_classes = 10\n",
    "n_samples = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_places_from_json(): loading .json file\n",
      "load_places_from_json(): loaded 208 items\n",
      "load_places_from_json(): loading .json file\n",
      "load_places_from_json(): loaded 208 items\n"
     ]
    }
   ],
   "source": [
    "# Blindfolded setup\n",
    "from torch.utils.data import DataLoader\n",
    "from blindflded_triplets_dataset import BlindfoldedTripletsDataset\n",
    "\n",
    "batch_size = 2\n",
    "sensor_key = 'accel_y' # selected for visualization\n",
    "sensor_keys = ['accel_x', 'accel_y', 'accel_z', 'rot_h', 'rot_p', 'rot_r']\n",
    "\n",
    "Blindfolded_Triplets_Dataset_train = BlindfoldedTripletsDataset(filepath = '../Places/PopulatedPlaces.json', sensor_keys=sensor_keys, n_selection=7, train=True)\n",
    "Blindfolded_Triplets_Dataset_test = BlindfoldedTripletsDataset(filepath = '../Places/PopulatedPlaces.json', sensor_keys=sensor_keys, n_selection=7, train=False)\n",
    "\n",
    "# sensor_idx = Blindfolded_Triplets_Dataset_train.tensor_index_translation(sensor_key)\n",
    "n_classes = int(max(Blindfolded_Triplets_Dataset_train.labels)) + 1\n",
    "n_samples = 6\n",
    "\n",
    "train_dataset = Blindfolded_Triplets_Dataset_train\n",
    "# test_dataset = Blindfolded_Triplets_Dataset_test\n",
    "\n",
    "n_samples = 6\n",
    "n_classes = int(max(train_dataset.labels)) + 1\n",
    "train_dataset = Blindfolded_Triplets_Dataset_train\n",
    "test_dataset = Blindfolded_Triplets_Dataset_test\n",
    "\n",
    "basic_dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "basic_dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BlindfoldedMLP(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(BlindfoldedMLP, self).__init__()\n",
    "        self.input_dim = 6 * 2400  # Fixed input size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure x is properly reshaped\n",
    "        x = x.view(x.shape[0], -1)  # Flatten (batch_size, 6, 2400) -> (batch_size, 14400)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 2400])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_series = train_dataset.__getitem__(0)\n",
    "\n",
    "# print(train_dataset.data)\n",
    "# print(test_series)\n",
    "\n",
    "print(test_series[0].shape)\n",
    "# for attr in dir(train_dataset)[:]:\n",
    "#     print(f\"{attr}: {type(getattr(train_dataset, attr))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phkunze/opt/anaconda3/envs/going_blindfolded/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'batch_idx' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m log_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics\u001b[38;5;241m=\u001b[39m[AverageNonzeroTripletsMetric()])\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# fit(basic_dataloader_train, basic_dataloader_test, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Plotting\u001b[39;00m\n\u001b[1;32m     45\u001b[0m train_embeddings_otl, train_labels_otl \u001b[38;5;241m=\u001b[39m extract_embeddings(online_train_loader, model)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Uni/IFL/Going Blindfolded/blindfolded/siamese-triplet/trainer.py:23\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics, start_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train stage\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m train_loss, metrics \u001b[38;5;241m=\u001b[39m train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)\n\u001b[1;32m     25\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Train set: Average loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, n_epochs, train_loss)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Uni/IFL/Going Blindfolded/blindfolded/siamese-triplet/trainer.py:90\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28mprint\u001b[39m(message)\n\u001b[1;32m     88\u001b[0m         losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 90\u001b[0m total_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss, metrics\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'batch_idx' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Set up the network and training parameters\n",
    "from networks import EmbeddingNet\n",
    "from losses import OnlineTripletLoss\n",
    "from utils import AllTripletSelector,HardestNegativeTripletSelector, RandomNegativeTripletSelector, SemihardNegativeTripletSelector # Strategies for selecting triplets within a minibatch\n",
    "from metrics import AverageNonzeroTripletsMetric\n",
    "from datasets import BalancedBatchSampler\n",
    "\n",
    "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each class\n",
    "train_batch_sampler = BalancedBatchSampler(train_dataset.train_labels, n_classes=n_classes, n_samples=n_samples)\n",
    "test_batch_sampler = BalancedBatchSampler(test_dataset.test_labels, n_classes=10, n_samples=n_samples)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "online_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
    "online_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler, **kwargs)\n",
    "\n",
    "\n",
    "# for (data, target), batch_idx in enumerate(online_train_loader):\n",
    "#     # batch_idx is the index of the current batch\n",
    "#     # data is the input data for the current batch\n",
    "#     # target is the target labels for the current batch\n",
    "#     print(f'Batch index: {batch_idx}')\n",
    "#     print(f'Data shape: {data.shape}')\n",
    "#     print(f'Target shape: {target.shape}')\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "blindfolded_MLP = BlindfoldedMLP(hidden_dim=128, output_dim=64)\n",
    "model = blindfolded_MLP\n",
    "if cuda:\n",
    "    print(\"cuda\")\n",
    "    model.cuda()\n",
    "print(next(model.parameters()).device)\n",
    "loss_fn = OnlineTripletLoss(margin, RandomNegativeTripletSelector(margin))\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 2\n",
    "log_interval = 50\n",
    "\n",
    "# Training\n",
    "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])\n",
    "# fit(basic_dataloader_train, basic_dataloader_test, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])\n",
    "\n",
    "# Plotting\n",
    "train_embeddings_otl, train_labels_otl = extract_embeddings(online_train_loader, model)\n",
    "plot_embeddings(train_embeddings_otl, train_labels_otl, title=\"Training set\", class_labels=class_labels)\n",
    "val_embeddings_otl, val_labels_otl = extract_embeddings(online_test_loader, model)\n",
    "plot_embeddings(val_embeddings_otl, val_labels_otl, title=\"Training set\", class_labels=class_labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "going_blindfolded",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
